# RAG System for SEC Filing Analysis

A Retrieval-Augmented Generation (RAG) system for answering complex questions about Apple and Tesla's SEC 10-K filings using open-source LLMs.

## Features

✅ **Advanced Document Parsing**: Semantic chunking using Docling's native structure to preserve tables and context  
✅ **Document Ingestion**: Parse PDF documents with semantic chunking  
✅ **Vector Search**: Efficient similarity search using FAISS  
✅ **Re-ranking**: Cross-encoder based re-ranking for higher relevance  
✅ **Open-Source LLM**: Mistral-7B via Hugging Face (no token required)  
✅ **Source Citation**: Accurate citations with document names and page numbers  
✅ **Out-of-Scope Handling**: Intelligent filtering of unanswerable questions  
✅ **Cloud-Ready**: Fully runnable on Kaggle/Colab notebooks  

## System Architecture

```
PDF Documents (Apple 10-K, Tesla 10-K)
    ↓
Document Ingestor (Chunking + Metadata)
    ↓
Sentence-Transformers (Embeddings)
    ↓
FAISS Vector Database
    ↓
FAISS Retrieval (Top-15)
    ↓
Cross-Encoder Re-ranking (Top-5)
    ↓
LLM (Mistral-7B) + Custom Prompt
    ↓
Answer + Sources
```

## Installation

### Prerequisites
- Python 3.8+ (Tested on Python 3.12)
- CUDA/GPU (recommended for HF models, CPU also works but slower)
- 4GB+ RAM for embeddings, 14GB+ for LLM

- Local: Python 3.12
- Colab: Python 3.12

### Setup

1. **Clone the repository**:
   ```bash
   git clone <repo-url>
   cd naive_rag
   ```

2. **Create virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
    pip install -r requirements.txt
   ```

4. **Set Hugging Face token** (optional - not needed for Mistral-7B):
        - Create a file at `./.env.txt` with:
            ```
            HUGGINGFACE_HUB_TOKEN=your_token_here
            ```
        - Or set an environment variable directly:
            ```bash
            export HUGGINGFACE_HUB_TOKEN=your_token_here
            ```
        - Only required if using gated models (e.g., Phi-3, Llama-2)

5. **Place PDF documents** in the project root:
   - `10-Q4-2024-As-Filed.pdf` (Apple 10-K)
   - `tsla-20231231-gen.pdf` (Tesla 10-K)

   - for Kaggle notebook:
   - PDFs provided via Kaggle Dataset attachment


## Usage

### Quick Start

Run the evaluation on all 13 test questions:

```bash
python main.py --mode evaluate
```

This will:
1. Index both PDF documents
2. Save the index for future use
3. Answer all 13 test questions
4. Save results to `evaluation_results.json`

### Command-Line Options

```bash
python main.py --help
```

**Options**:
- `--mode {index,query,evaluate}`: Operation mode (default: evaluate)
- `--query TEXT`: Question to answer (required for query mode)
- `--model MODEL`: LLM model name (default: mistralai/Mistral-7B-Instruct-v0.2)
- `--embedding-model MODEL`: Embedding model (default: all-MiniLM-L6-v2)
- `--index-dir PATH`: Directory for saving/loading index (default: ./rag_index)

### Modes

**1. Index Mode** - Create and save the vector index:
```bash
python main.py --mode index --index-dir ./my_index
```

**2. Query Mode** - Answer a single question:
```bash
python main.py --mode query --query "What was Apple's total revenue in 2024?" --index-dir ./my_index
```

**3. Evaluate Mode** (default) - Answer all 13 test questions:
```bash
python main.py --mode evaluate
```

### Python API

```python
from rag_system import RAGSystem

# Initialize
rag = RAGSystem(model_name="mistralai/Mistral-7B-Instruct-v0.2", use_reranker=True)

# Ingest documents
documents = [
    {"path": "10-Q4-2024-As-Filed.pdf", "name": "Apple 10-K"},
    {"path": "tsla-20231231-gen.pdf", "name": "Tesla 10-K"}
]
rag.ingest_documents(documents)

# Answer a question
result = rag.answer_question("What was Apple's total revenue in 2024?")
print(result["answer"])
print(result["sources"])
```

### Using the Interface Function

```python
from rag_system.rag_system import RAGSystem

rag = RAGSystem()
# ... (index documents)

def answer_question(query: str) -> dict:
    """Answers a question using the RAG pipeline."""
    return rag.answer_question(query)

# Usage
result = answer_question("What was Tesla's revenue in 2023?")
# Returns: {
#     "answer": "Tesla's total revenue for 2023 was $96,773 million.",
#     "sources": ["Tesla 10-K, p. 45"]
# }
```

## Test Questions & Expected Answers

| Q# | Question | Expected Answer | Source |
|---|---|---|---|
| 1 | Apple's total revenue FY2024 | $391,036 million | Apple 10-K, Item 8, p. 282 |
| 2 | Apple common stock outstanding | 15,115,823,000 shares | Apple 10-K, first paragraph |
| 3 | Apple's total term debt | $96,662 million | Apple 10-K, Item 8, Note 9, p. 394 |
| 4 | Apple's 10-K filing date | November 1, 2024 | Apple 10-K, Signature page |
| 5 | Apple unresolved SEC comments | No (checkmark under Item 1B) | Apple 10-K, Item 1B, p. 176 |
| 6 | Tesla's total revenue FY2023 | $96,773 million | Tesla 10-K, Item 7 |
| 7 | Tesla Automotive revenue % | ~84% ($81,924M/$96,773M) | Tesla 10-K, Item 7 |
| 8 | Tesla's Elon Musk dependency | Central to strategy, innovation, leadership | Tesla 10-K, Item 1A |
| 9 | Tesla vehicle models | Model S, 3, X, Y, Cybertruck | Tesla 10-K, Item 1 |
| 10 | Tesla lease pass-through | Finance solar systems; customers sign PPAs | Tesla 10-K, Item 7 |
| 11 | Tesla stock price forecast 2025 | **Out of scope** | N/A |
| 12 | Apple CFO 2025 | **Out of scope** | N/A |
| 13 | Tesla HQ color | **Out of scope** | N/A |

## Output Format

Answers are returned in JSON format:

```json
[
  {
    "question_id": 1,
    "answer": "Apple's total revenue for fiscal year 2024 was $391,036 million.",
    "sources": ["Apple 10-K, p. 282"]
  },
  {
    "question_id": 11,
    "answer": "This question cannot be answered based on the provided documents.",
    "sources": []
  }
]
```

## Cloud Deployment (Kaggle/Colab)

1. **Create notebook** in Kaggle or Colab
2. **Clone repository**:
   ```bash
   !git clone <repo-url>
   %cd naive_rag
   ```
3. **Install dependencies**:
   ```bash
   !pip install -q -r requirements.txt
   ```
4. **Set Hugging Face token** (required for gated models):
    ```bash
    import os
    os.environ["HUGGINGFACE_HUB_TOKEN"] = "your_token_here"
    ```
5. **Run RAG system**:
   ```bash
   !python main.py --mode evaluate
   ```

## Design Details

See [design.md](design.md) for detailed information on:
- Chunking strategy and rationale
- Embedding model selection
- Re-ranking pipeline justification
- Out-of-scope question detection
- LLM integration approach

## Performance

- **Embedding Speed**: ~1000 vectors/sec
- **FAISS Search**: <50ms per query
- **Re-ranking**: ~200ms for 15 candidates
- **LLM Response**: 2-5 seconds
- **Total Latency**: 2-6 seconds per question

## Project Structure

```
naive_rag/
├── rag_system/
│   ├── __init__.py              # Package initialization
│   ├── ingestion.py             # Document parsing & embedding
│   ├── retriever.py             # Vector search & re-ranking
│   ├── llm_integration.py       # LLM prompting & generation
│   └── rag_system.py            # Main orchestrator
├── main.py                      # CLI entry point
├── requirements.txt             # Python dependencies
├── design.md                    # System design report
├── README.md                    # This file
└── notebooks/
    └── rag_demo.ipynb           # Jupyter notebook demo
```

## Troubleshooting

### Issue: "No module named 'docling'"
**Solution**: `pip install docling`

### Issue: "Repository not found" or model access denied
**Solution**:
- Ensure you accepted the model license on Hugging Face
- Set `HUGGINGFACE_HUB_TOKEN` before running

### Issue: Slow embedding generation
**Solution**:
- Use smaller embedding model: `--embedding-model sentence-transformers/all-MiniLM-L6-v2`
- Enable GPU: Ensure CUDA is properly installed

### Issue: LLM responses are too long/off-topic
**Solution**:
- Lower temperature: `--temperature 0.2` (in code)
- Increase retrieved context filtering

## Benchmarks

Tested on:
- ✅ Kaggle GPU (P100)
- ✅ Google Colab T4
- ✅ Local M1 MacBook (CPU, slower)
- ✅ Linux server (CPU+GPU)

## Future Enhancements

- [ ] Hybrid BM25 + semantic search
- [ ] Multi-hop reasoning for complex questions
- [ ] Query expansion and reformulation
- [ ] Fine-tuned domain-specific LLM
- [ ] Web interface (Streamlit/Gradio)
- [ ] Support for more SEC filings

## License

MIT License - See LICENSE file for details

## Citation

If you use this RAG system, please cite:

```bibtex
@software{rag_sec_filings,
  title={RAG System for SEC Filing Analysis},
  author={Your Name},
  year={2024},
  url={https://github.com/youruser/naive_rag}
}
```

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## Support

For issues, questions, or suggestions:
- Open an GitHub issue
- Check existing documentation
- Review [design.md](design.md) for architecture details

---

**Built with**: Docling • Sentence-Transformers • FAISS • Hugging Face Transformers • Mistral-7B
